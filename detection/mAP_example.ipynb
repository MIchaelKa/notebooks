{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acute-ebony",
   "metadata": {},
   "source": [
    "## mAP на примере одного батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "varying-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "opponent-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intersection(set_1, set_2):\n",
    "    \"\"\"\n",
    "    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n",
    "\n",
    "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
    "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
    "    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
    "    \"\"\"\n",
    "\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n",
    "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n",
    "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n",
    "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n",
    "\n",
    "\n",
    "def find_jaccard_overlap(set_1, set_2):\n",
    "    \"\"\"\n",
    "    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n",
    "\n",
    "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
    "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
    "    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
    "    \"\"\"\n",
    "\n",
    "    # Find intersections\n",
    "    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n",
    "\n",
    "    # Find areas of each box in both sets\n",
    "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n",
    "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n",
    "\n",
    "    # Find the union\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n",
    "\n",
    "    return intersection / union  # (n1, n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-complaint",
   "metadata": {},
   "source": [
    "### Init some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cosmetic-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_boxes = [\n",
    "    torch.tensor([[0.5181, 0.1481, 0.9734, 0.9495],\n",
    "        [0.4061, 0.0868, 0.9967, 1.0121]]),\n",
    "    torch.tensor([[0.5112, 0.2552, 0.9611, 0.6476],\n",
    "        [0.3756, 0.2327, 0.9368, 0.5786],\n",
    "        [0.1407, 0.2285, 0.7031, 0.6167],\n",
    "        [0.2269, 0.2269, 0.8302, 0.5679]]),\n",
    "    torch.tensor([[0.0262, 0.0626, 0.9048, 1.0039]]),\n",
    "    torch.tensor([]),\n",
    "    torch.tensor([[0.1899, 0.2915, 0.8136, 0.9690],\n",
    "         [0.2999, 0.2476, 0.8813, 0.9457],\n",
    "         [0.0913, 0.3507, 0.6562, 0.9572],\n",
    "         [0.0658, 0.2665, 0.7552, 0.9305]]),\n",
    "    torch.tensor([[0.0594, 0.0101, 0.7279, 1.0358],\n",
    "         [0.1577, 0.1561, 0.8410, 0.9924],\n",
    "         [0.0226, 0.1019, 0.5832, 0.9948]]),\n",
    "    torch.tensor([[0.2565, 0.3249, 0.8049, 0.8538],\n",
    "         [0.1724, 0.3774, 0.7303, 0.8684]]),\n",
    "    torch.tensor([[ 0.0885,  0.0042,  0.7078,  0.9169],\n",
    "         [ 0.1837, -0.0027,  0.8143,  0.9832],\n",
    "         [ 0.0447,  0.0715,  0.5369,  0.8338]])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unlimited-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_scores = [\n",
    "    torch.tensor([0.7783, 0.5865]),\n",
    "    torch.tensor([0.8726, 0.8634, 0.7082, 0.6611]),\n",
    "    torch.tensor([0.9984]),\n",
    "    torch.tensor([]),\n",
    "    torch.tensor([0.9895, 0.9589, 0.9230, 0.7901]),\n",
    "    torch.tensor([0.9995, 0.6241, 0.5843]),\n",
    "    torch.tensor([0.9849, 0.6808]),\n",
    "    torch.tensor([0.9874, 0.8269, 0.5664])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "individual-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_boxes = [\n",
    "    torch.tensor([[0.5580, 0.0693, 0.9980, 0.9920]]),\n",
    "    torch.tensor([[0.0000, 0.2480, 0.4640, 0.6853],\n",
    "        [0.5280, 0.2533, 0.9960, 0.6987]]),\n",
    "    torch.tensor([[0.0080, 0.0675, 0.9640, 0.9775]]),\n",
    "    torch.tensor([[0.5300, 0.4933, 0.6680, 0.5413]]),\n",
    "    torch.tensor([[0.0000, 0.1680, 0.8260, 0.8853]]),\n",
    "    torch.tensor([[0.0080, 0.0150, 0.8160, 0.9970]]),\n",
    "    torch.tensor([[0.2643, 0.4604, 0.5375, 0.8562],\n",
    "        [0.3754, 0.3250, 0.7267, 0.8188]]),\n",
    "    torch.tensor([[0.0000, 0.0000, 0.8053, 0.9620]])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "treated-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [\n",
    "    torch.tensor([9]),\n",
    "    torch.tensor([20, 20]),\n",
    "    torch.tensor([12]),\n",
    "    torch.tensor([4]),\n",
    "    torch.tensor([15]),\n",
    "    torch.tensor([15]),\n",
    "    torch.tensor([13, 13]),\n",
    "    torch.tensor([14])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "diagnostic-doctrine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 8, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(det_boxes), len(det_scores), len(true_boxes), len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adaptive-collect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4]), torch.Size([2, 4]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different numbers of predictions and true objects\n",
    "det_boxes[1].shape, true_boxes[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-acoustic",
   "metadata": {},
   "source": [
    "### true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "monetary-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_images = list()\n",
    "for i in range(len(true_labels)):\n",
    "    true_images.extend([i] * true_labels[i].size(0))\n",
    "# (n_objects), n_objects is the total no. of objects across all images\n",
    "true_images = torch.LongTensor(true_images)\n",
    "true_boxes = torch.cat(true_boxes, dim=0)  # (n_objects, 4)\n",
    "true_labels = torch.cat(true_labels, dim=0)  # (n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "subsequent-dealer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 2, 3, 4, 5, 6, 6, 7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "executive-increase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 20, 20, 12,  4, 15, 15, 13, 13, 14])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "instrumental-guyana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10]), torch.Size([10, 4]), torch.Size([10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_images.shape, true_boxes.shape, true_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-traveler",
   "metadata": {},
   "source": [
    "### det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "animated-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_images = list()\n",
    "for i in range(len(det_scores)):\n",
    "    det_images.extend([i] * det_scores[i].size(0))\n",
    "det_images = torch.LongTensor(det_images) # (n_detections)\n",
    "det_boxes = torch.cat(det_boxes, dim=0)  # (n_detections, 4)\n",
    "det_scores = torch.cat(det_scores, dim=0)  # (n_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "favorite-rebel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 1, 1, 2, 4, 4, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "middle-advancement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7783, 0.5865, 0.8726, 0.8634, 0.7082, 0.6611, 0.9984, 0.9895, 0.9589,\n",
       "        0.9230, 0.7901, 0.9995, 0.6241, 0.5843, 0.9849, 0.6808, 0.9874, 0.8269,\n",
       "        0.5664])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "scientific-botswana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([19]), torch.Size([19, 4]), torch.Size([19]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_images.shape, det_boxes.shape, det_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-breathing",
   "metadata": {},
   "source": [
    "### steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pending-operations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_objects = true_boxes.size(0)\n",
    "n_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "registered-watch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8), torch.Size([10]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_boxes_detected = torch.zeros(n_objects, dtype=torch.uint8)\n",
    "true_boxes_detected, true_boxes_detected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "incorporate-ridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_detections = det_boxes.size(0)\n",
    "n_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interracial-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort detections in decreasing order of confidence/scores\n",
    "det_scores_sorted, sort_ind = torch.sort(det_scores, dim=0, descending=True)  # (n_detections)\n",
    "det_images_sorted = det_images[sort_ind]  # (n_detections)\n",
    "det_boxes_sorted = det_boxes[sort_ind]  # (n_detections, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "breathing-allen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9995, 0.9984, 0.9895, 0.9874, 0.9849, 0.9589, 0.9230, 0.8726, 0.8634,\n",
       "        0.8269, 0.7901, 0.7783, 0.7082, 0.6808, 0.6611, 0.6241, 0.5865, 0.5843,\n",
       "        0.5664])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_scores_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rough-albany",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 2, 4, 7, 6, 4, 4, 1, 1, 7, 4, 0, 1, 6, 1, 5, 0, 5, 7])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_images_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acute-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the order of decreasing scores, check if true or false positive\n",
    "true_positives = torch.zeros((n_detections), dtype=torch.float) # (n_detections)\n",
    "false_positives = torch.zeros((n_detections), dtype=torch.float) # (n_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "partial-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives = torch.zeros((n_detections), dtype=torch.float)  # (n_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "personal-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "for d in range(n_detections):\n",
    "    this_detection_box = det_boxes_sorted[d].unsqueeze(0)  # (1, 4)\n",
    "    this_image = det_images_sorted[d]  # (), scalar\n",
    "    \n",
    "    # print(this_detection_box.shape, this_image)\n",
    "\n",
    "    # Find objects in the same image and whether they have been detected before\n",
    "    \n",
    "    this_image_boxes = true_boxes[true_images==this_image] # (n_objects_in_img, 4)\n",
    "    \n",
    "    # Find maximum overlap of this detection with objects in this image of this class\n",
    "    overlaps = find_jaccard_overlap(this_detection_box, this_image_boxes)  # (1, n_objects_in_img)\n",
    "    max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)  # (), () - scalars\n",
    "    \n",
    "    # Index in the true_boxes and true_boxes_detected to find duplicated detections\n",
    "    original_ind = torch.LongTensor(range(true_boxes.size(0)))[true_images == this_image][ind]\n",
    "    \n",
    "    if max_overlap > threshold:      \n",
    "        if true_boxes_detected[original_ind] == 0:\n",
    "            true_positives[d] = 1\n",
    "            true_boxes_detected[original_ind] = 1\n",
    "        else:\n",
    "            false_positives[d] = 1\n",
    "    else:\n",
    "        false_positives[d] = 1\n",
    "        \n",
    "    false_negatives[d] = (1-true_boxes_detected).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "sustainable-edmonton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "statutory-communications",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "median-tower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 8., 7., 6., 5., 5., 5., 4., 4., 4., 4., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "exposed-deviation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_boxes_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "continent-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cumulative precision and recall at each detection in the order of decreasing scores\n",
    "cumul_true_positives = torch.cumsum(true_positives, dim=0)  # (n_detections)\n",
    "cumul_false_positives = torch.cumsum(false_positives, dim=0)  # (n_detections)\n",
    "# cumul_false_negatives = torch.cumsum(false_negatives, dim=0)  # (n_detections)\n",
    "\n",
    "cumul_precision = cumul_true_positives / (\n",
    "    cumul_true_positives + cumul_false_positives + 1e-10)  # (n_detections)\n",
    "\n",
    "cumul_recall = cumul_true_positives / (\n",
    "    cumul_true_positives + false_negatives + 1e-10)  # (n_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "beneficial-allah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 5., 5., 6., 6., 6., 6., 7., 7., 7., 7., 7., 7., 7.,\n",
       "         7.]),\n",
       " tensor([ 0.,  0.,  0.,  0.,  0.,  1.,  2.,  2.,  3.,  4.,  5.,  5.,  6.,  7.,\n",
       "          8.,  9., 10., 11., 12.]),\n",
       " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8333, 0.7143, 0.7500, 0.6667,\n",
       "         0.6000, 0.5455, 0.5833, 0.5385, 0.5000, 0.4667, 0.4375, 0.4118, 0.3889,\n",
       "         0.3684]),\n",
       " tensor([0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.5000, 0.5000, 0.6000, 0.6000,\n",
       "         0.6000, 0.6000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000,\n",
       "         0.7000]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumul_true_positives, cumul_false_positives, cumul_precision, cumul_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "falling-justice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.10000000149011612,\n",
       " 0.20000000298023224,\n",
       " 0.30000001192092896,\n",
       " 0.4000000059604645,\n",
       " 0.5,\n",
       " 0.6000000238418579,\n",
       " 0.699999988079071,\n",
       " 0.800000011920929,\n",
       " 0.8999999761581421,\n",
       " 1.0]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_thresholds = torch.arange(start=0, end=1.1, step=.1).tolist()  # (11)\n",
    "recall_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "periodic-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float)  # (11)\n",
    "for i, t in enumerate(recall_thresholds):\n",
    "    recalls_above_t = cumul_recall >= t\n",
    "    if recalls_above_t.any():\n",
    "        precisions[i] = cumul_precision[recalls_above_t].max()\n",
    "    else:\n",
    "        precisions[i] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "premium-coast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7500, 0.5833, 0.0000,\n",
       "        0.0000, 0.0000])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "tropical-member",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6667)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision = precisions.mean()\n",
    "average_precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
