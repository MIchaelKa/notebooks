{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ImageNet input - 3 channels, 224x224,\n",
    "# values don't matter as we care about network structure.\n",
    "# But they can also be real inputs.\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "dummy_input_2 = torch.randn(1, 600, 600, 3)\n",
    "dummy_input_3 = torch.randn(1, 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain your model, it can be also constructed in your script explicitly\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "model = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.resnet18(pretrained=True)   \n",
    "\n",
    "    def forward(self, x): \n",
    "        # x = torch.permute(x, (0, 3, 1, 2))\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc687b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cf70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_output = model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230dc9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec5b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Computes the number of trainable parameters\n",
    "#\n",
    "# Sourse:\n",
    "# https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325\n",
    "#\n",
    "def get_num_of_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_of_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287077ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_of_model(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "        \n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    \n",
    "    print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51de090",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_size_of_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da9b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke export\n",
    "input_names = [\"input_1\"]\n",
    "output_names = [\"output_1\"]\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input, \n",
    "    \"models/resnet18chw.onnx\",\n",
    "    opset_version=12,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116d100-125d-48d3-826b-293beba7c6e9",
   "metadata": {},
   "source": [
    "# ONNX to ORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40e973-ee0e-4aa2-a64c-e845483fd84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m onnxruntime.tools.convert_onnx_models_to_ort --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8091d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME=\"./models/resnet18.onnx\"\n",
    "MODEL_NAME='/Users/user006/Developer/models/yunet/yan_1class.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m onnxruntime.tools.convert_onnx_models_to_ort {MODEL_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747fcdc-2910-499f-bbe7-9369a08ea638",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m onnxruntime.tools.convert_onnx_models_to_ort ./models/idp/resnet18_mobile_numbers_quantized/resnet18_mobile_numbers_quantized.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd6296c",
   "metadata": {},
   "source": [
    "# Inspecting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae71f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"models/capturing_2305.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81627f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a human readable representation of the graph\n",
    "# print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../capturing/ && pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../capturing/ && python inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d836f",
   "metadata": {},
   "source": [
    "# pytorch to coreml\n",
    "https://coremltools.readme.io/docs/pytorch-conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fdcf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e2c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f905618",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, dummy_input)\n",
    "out = traced_model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479948c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traced_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7366cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using image_input in the inputs parameter:\n",
    "# Convert to Core ML using the Unified Conversion API.\n",
    "coreml_model = ct.convert(\n",
    "    traced_model,\n",
    "#     inputs=[ct.TensorType(shape=dummy_input.shape)]\n",
    "    inputs=[ct.ImageType(shape=dummy_input.shape)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the converted model.\n",
    "coreml_model.save(\"resnet18.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3008b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "barcode_eval_venv",
   "language": "python",
   "name": "barcode_eval_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
