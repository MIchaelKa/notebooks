{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c69ea-5bf5-48e5-8071-49ae63df3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58d879-7390-4359-a6a1-92cb1ccc728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9dfe86-47bf-4257-a230-497ecd9dee98",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255cd355-ff88-4630-a6cc-5f9fab30d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0843a7c-507d-4507-bb01-83229cfac4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92effdbe-8ff6-41b7-97ae-704ba5c71f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/19839701/Developer/datasets'\n",
    "MRZ_PATH = '/Users/19839701/Developer/datasets/mrz'\n",
    "\n",
    "images_path = os.path.join(DATA_PATH, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c3840-751d-4fb1-aeba-8554ae4c878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/user006/Developer/temp/paddle2onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead972b9-a6f7-4d87-822a-83e1078ce29e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PaddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf6d74-abc7-4d55-af0f-6edc9d6e0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar\n",
    "# to /Users/19839701/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d48a27-fb3a-40fe-bc62-8bee8c03ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rec_model_dir='/Users/19839701/Developer/python/ipynb/models/en_PP-OCRv3_rec_slim_infer'\n",
    "rec_model_dir='/Users/19839701/Developer/python/ipynb/models/en_PP-OCRv3_rec_infer'\n",
    "# rec_model_dir='/Users/19839701/Developer/python/ipynb/models/en_number_mobile_v2.0_rec_slim_infer'\n",
    "\n",
    "rec_image_shape='3, 32, 320'\n",
    "\n",
    "ocr = PaddleOCR(lang='en', rec_model_dir=rec_model_dir, rec_image_shape=rec_image_shape)\n",
    "\n",
    "#ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "\n",
    "# ocr = PaddleOCR(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893cf27-7b46-4321-9c3e-a1faef71ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = os.path.join(images_path, 'text-2-600.png')\n",
    "image_path = os.path.join(images_path, 'ppocr_img/imgs_words/en/word_1.png')\n",
    "# image_path = os.path.join(images_path, 'ppocr_img/imgs_words/russia/ru_1.jpg')\n",
    "\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f1018-ada6-4255-ba1d-ad7f073000b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "result = ocr.ocr(image_path, det=False, cls=False, rec=True)\n",
    "print(f\"time: {time.time() - t0}\")\n",
    "\n",
    "for line in result:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ecc52-68c6-45bd-8a24-a0da3491906b",
   "metadata": {},
   "source": [
    "# Check version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf1ef71-df4e-4373-a54d-0803787167a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Versions for compatibility with onnxrt 1.7.2\n",
    "#\n",
    "\n",
    "# pip install paddlepaddle==2.3.1\n",
    "# pip install paddleocr==2.5.0.3\n",
    "# pip install onnxruntime==1.7.0\n",
    "# pip install onnx==1.8.1\n",
    "# pip install paddle2onnx==0.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a654b013-3107-4bfe-ad25-9a0472c26780",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# paddle2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d031d7d-665a-418a-bfa5-1bce56179569",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29770b24-0046-43f5-b3b1-4eae30925894",
   "metadata": {},
   "source": [
    "## en_PP-OCRv3_det_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7a802d-f2ee-41d7-8037-1f9972ccb042",
   "metadata": {},
   "outputs": [],
   "source": [
    "!paddle2onnx --model_dir ./en_PP-OCRv3_det_infer \\\n",
    "--model_filename inference.pdmodel \\\n",
    "--params_filename inference.pdiparams \\\n",
    "--save_file ./en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.onnx \\\n",
    "--opset_version 10 \\\n",
    "--enable_onnx_checker True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16931bf2-3e1d-40d7-8312-fde29f94b1cb",
   "metadata": {},
   "source": [
    "## en_PP-OCRv3_rec_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae20cd9-00bc-4ad3-9a47-4a4773c0e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "!paddle2onnx --model_dir ./models/en_PP-OCRv3_rec_infer \\\n",
    "--model_filename inference.pdmodel \\\n",
    "--params_filename inference.pdiparams \\\n",
    "--save_file ./models/en_PP-OCRv3_rec_infer/en_PP-OCRv3_rec_infer.onnx \\\n",
    "--opset_version 10 \\\n",
    "--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n",
    "--enable_onnx_checker True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9fa89f-2ad1-4131-b476-05b3727a6734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paddle2onnx==0.8.1\n",
    "!paddle2onnx --model_dir ./models/en_PP-OCRv3_rec_infer \\\n",
    "--model_filename inference.pdmodel \\\n",
    "--params_filename inference.pdiparams \\\n",
    "--save_file ./models/en_PP-OCRv3_rec_infer/en_PP-OCRv3_rec_infer.onnx \\\n",
    "--opset_version 10 \\\n",
    "--enable_onnx_checker True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ffc44a-4837-49f8-991b-76550129db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!paddle2onnx --model_dir ./models/en_PP-OCRv3_rec_slim_infer \\\n",
    "--model_filename inference.pdmodel \\\n",
    "--params_filename inference.pdiparams \\\n",
    "--save_file ./models/en_PP-OCRv3_rec_slim_infer/model.onnx \\\n",
    "--opset_version 10 \\\n",
    "--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n",
    "--enable_onnx_checker True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae0f3f-7cd4-45c7-a4ed-a51397df4790",
   "metadata": {},
   "source": [
    "## en_number_mobile_v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d960bd7-18d3-43f7-8cf0-43f166babaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!paddle2onnx --model_dir ./models/en_number_mobile_v2.0_rec_infer \\\n",
    "--model_filename inference.pdmodel \\\n",
    "--params_filename inference.pdiparams \\\n",
    "--save_file ./models/en_number_mobile_v2.0_rec_infer/en_number_mobile_v2.0_rec_infer.onnx \\\n",
    "--opset_version 10 \\\n",
    "--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n",
    "--enable_onnx_checker True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64669e6-bf90-4c43-ab7a-57619dc5232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!paddle2onnx --model_dir ./models/en_number_mobile_v2.0_rec_slim_infer \\\n",
    "--model_filename inference.pdmodel \\\n",
    "--params_filename inference.pdiparams \\\n",
    "--save_file ./models/en_number_mobile_v2.0_rec_slim_infer/en_number_mobile_v2.0_rec_slim_infer.onnx \\\n",
    "--opset_version 10 \\\n",
    "--input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n",
    "--enable_onnx_checker True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3cc25-290d-4246-bcef-8ba784349c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paddle2onnx==0.8.1\n",
    "\n",
    "!paddle2onnx --model_dir ./models/en_number_mobile_v2.0_rec_infer \\\n",
    "--model_filename inference.pdmodel \\\n",
    "--params_filename inference.pdiparams \\\n",
    "--save_file ./models/en_number_mobile_v2.0_rec_infer/en_number_mobile_v2.0_rec_infer.onnx \\\n",
    "--opset_version 10 \\\n",
    "--enable_onnx_checker True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a456fcf-7fbb-4083-a299-4dca1f6eebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!paddle2onnx --model_dir ./models/rec_en_number_lite_pt_infer \\\n",
    "--model_filename inference.pdmodel \\\n",
    "--params_filename inference.pdiparams \\\n",
    "--save_file ./models/rec_en_number_lite_pt_infer/rec_en_number_lite_pt_infer.onnx \\\n",
    "--opset_version 10 \\\n",
    "--enable_onnx_checker True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae5661-e629-4539-b40c-23d2a202fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!paddle2onnx --model_dir ./models/en_number_mobile_v2.0_rec_train_infer \\\n",
    "--model_filename inference.pdmodel \\\n",
    "--params_filename inference.pdiparams \\\n",
    "--save_file ./models/en_number_mobile_v2.0_rec_train_infer/en_number_mobile_v2.0_rec_train_infer.onnx \\\n",
    "--opset_version 10 \\\n",
    "--enable_onnx_checker True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a75a13-ce49-4a8c-8925-b8ceb262b3f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19f94a-0221-4f62-853f-2ad3ce760599",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"./models/en_number_mobile_v2.0_rec_infer/en_number_mobile_v2.0_rec_infer.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37445cb-a245-44b6-ad5e-f9f16b2fb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m onnxruntime.tools.convert_onnx_models_to_ort {MODEL_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e018dd-1b1e-4e65-8988-bad320f6ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /Users/19839701/Developer/projects/onnxruntime-dev/onnxruntime-1.7.2/tools/python/convert_onnx_models_to_ort.py /Users/19839701/Developer/python/ipynb/models/en_number_mobile_v2.0_rec_infer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9fb281-7921-40e0-a0cd-5a9b9bba19a8",
   "metadata": {},
   "source": [
    "# Inspecting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59324d0-1e38-40dc-bf79-7e61c8297249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eee1c8-a594-41d1-9185-15c868ba4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './models/mobilenetv3_quant_no_preproc.onnx'\n",
    "onnx_model = onnx.load(model_path)\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7adb49a-cfee-4016-8a68-2b70f4c7f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaccb064-6959-43f6-8a46-75d6a636e157",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345bea2c-dc26-479f-9165-742889da8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output/v4/rec_en_number_lite_pt/best_accuracy\n",
    "python3 -m paddle.distributed.launch --gpus '1' tools/eval.py \\\n",
    "-c configs/rec/multi_language/rec_en_number_lite_train_mrz.yml \\\n",
    "-o Global.checkpoints=output/v4/rec_en_number_lite_pt/best_accuracy \\\n",
    "Global.use_visualdl=False \\\n",
    "Global.use_space_char=True \\\n",
    "Global.character_dict_path=ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474a3d6-d896-4f73-a54b-d9b161636b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_models/en_number_mobile_v2.0_rec_train\n",
    "python3 -m paddle.distributed.launch --gpus '1' tools/eval.py \\\n",
    "-c configs/rec/multi_language/rec_en_number_lite_train_mrz.yml \\\n",
    "-o Global.checkpoints=pretrain_models/en_number_mobile_v2.0_rec_train/best_accuracy \\\n",
    "Global.use_visualdl=False \\\n",
    "Global.use_space_char=True \\\n",
    "Global.character_dict_path=ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e22b3c-0995-4c4d-bc5a-3e877e7e54f7",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38786d95-a5aa-4fb4-8b40-81cc4c7190f7",
   "metadata": {},
   "source": [
    "## Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a0312-bf54-473f-b97b-bd6108e880d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "./en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e838684-81c7-42dd-906d-5ac7cb10274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tools/infer/predict_det.py \\\n",
    "--use_gpu=False \\\n",
    "--image_dir=\"./doc/imgs_en/img_12.jpg\" \\\n",
    "--det_model_dir=\"./models/en_PP-OCRv3_det_infer/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042d659-e083-4014-95ef-a44cadacaf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX\n",
    "python tools/infer/predict_det.py \\\n",
    "--use_gpu=False \\\n",
    "--use_onnx=True \\\n",
    "--image_dir=\"./doc/imgs_en/img_12.jpg\" \\\n",
    "--det_model_dir=\"./models/en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.onnx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dffd6e-af63-4e5c-95f8-6b7c14f2b30e",
   "metadata": {},
   "source": [
    "## Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2079b8-ad05-44d0-9745-f614539ca186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output/v4/rec_en_number_lite_pt/rec_en_number_lite_pt_infer\n",
    "python tools/infer/predict_rec.py --use_gpu=False \\\n",
    "--rec_model_dir=./output/v4/rec_en_number_lite_pt/rec_en_number_lite_pt_infer  \\\n",
    "--image_dir=./train_data/crop_midv500/valid/word_100.jpg \\\n",
    "--rec_image_shape='3,32,320' \\\n",
    "--rec_char_dict_path=./ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5107d4-181e-4a82-bb8a-1d44ae07339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_models/en_number_mobile_v2.0_rec_train/en_number_mobile_v2.0_rec_train_infer\n",
    "python tools/infer/predict_rec.py --use_gpu=False \\\n",
    "--rec_model_dir=./pretrain_models/en_number_mobile_v2.0_rec_train/en_number_mobile_v2.0_rec_train_infer  \\\n",
    "--image_dir=./train_data/crop_midv500/valid/word_100.jpg \\\n",
    "--rec_image_shape='3,32,320' \\\n",
    "--rec_char_dict_path=./ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c8216-3c89-4039-af60-882ccfcb2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test new\n",
    "python tools/infer/predict_rec.py --use_gpu=False \\\n",
    "--rec_model_dir=./output/v4/rec_en_number_lite_pt/rec_en_number_lite_pt_infer  \\\n",
    "--image_dir=./train_data/crop_midv500/valid \\\n",
    "--rec_image_shape='3,32,320' \\\n",
    "--rec_char_dict_path=./ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ac362f-c4b8-405e-b753-a5a41a24f75f",
   "metadata": {},
   "source": [
    "# Inference ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37217729-3930-4eee-8078-97efd251d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "# cd /Users/19839701/Developer/python/PaddleOCR\n",
    "# source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8258ed-b21c-4106-8277-3bdba8d33c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rec_en_number_lite_pt_infer.onnx\n",
    "python tools/infer/predict_rec.py --use_gpu=False --use_onnx=True \\\n",
    "--rec_model_dir=/Users/19839701/Developer/python/paddle_ocr_utils/models/rec_en_number_lite_pt_infer/rec_en_number_lite_pt_infer.onnx \\\n",
    "--image_dir=/Users/19839701/Developer/python/smartcv-sdk-rnd/data/mrz/crop_midv500/valid/word_100.jpg \\\n",
    "--rec_image_shape='3,32,320' \\\n",
    "--rec_char_dict_path=./ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e79bd4-64d9-4c84-b87b-ea7039f431d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_number_mobile_v2.0_rec_train_infer.onnx\n",
    "python tools/infer/predict_rec.py --use_gpu=False --use_onnx=True \\\n",
    "--rec_model_dir=/Users/19839701/Developer/python/paddle_ocr_utils/models/en_number_mobile_v2.0_rec_train_infer/en_number_mobile_v2.0_rec_train_infer.onnx \\\n",
    "--image_dir=/Users/19839701/Developer/python/smartcv-sdk-rnd/data/mrz/crop_midv500/valid/word_100.jpg \\\n",
    "--rec_image_shape='3,32,320' \\\n",
    "--rec_char_dict_path=./ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a92b2-ef70-40a4-87aa-9d68b0284385",
   "metadata": {},
   "source": [
    "# Inference old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1833c1-e3f2-47d0-a362-2fc5922c1479",
   "metadata": {},
   "source": [
    "## en_PP-OCRv3_rec_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c783dc-42af-437d-a97a-caf83ee6a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../PaddleOCR/tools/infer/predict_system.py --use_gpu=False --use_onnx=True \\\n",
    "--rec_model_dir=./models/en_PP-OCRv3_rec_infer/model.onnx  \\\n",
    "--image_dir=/Users/19839701/Developer/datasets/images/ppocr_img/imgs_words/russia/ru_1.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3e6163-3d9c-4239-b012-edb91058a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../PaddleOCR/tools/infer/predict_rec.py --use_gpu=False --use_onnx=True \\\n",
    "--rec_model_dir=./models/en_PP-OCRv3_rec_infer/model.onnx  \\\n",
    "--image_dir=/Users/19839701/Developer/datasets/images/ppocr_img/imgs_words/russia/ru_1.jpg \\\n",
    "--rec_char_dict_path=../PaddleOCR/ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a07e5-736d-4f37-88b5-41c2a48e1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../PaddleOCR/tools/infer/predict_rec.py --use_gpu=False --use_onnx=True \\\n",
    "--rec_model_dir=./models/en_PP-OCRv3_rec_infer/model.onnx  \\\n",
    "--image_dir=/Users/19839701/Developer/datasets/images/ppocr_img/imgs_words/en/word_1.png \\\n",
    "--rec_char_dict_path=../PaddleOCR/ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8429ff-8499-4a3b-b1be-ca1e108cd352",
   "metadata": {},
   "source": [
    "## en_number_mobile_v2.0_rec_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d29f8ad-1e7f-4a12-a43b-7d99bfbff846",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../PaddleOCR/tools/infer/predict_rec.py --use_gpu=False \\\n",
    "--rec_model_dir=./models/en_number_mobile_v2.0_rec_infer  \\\n",
    "--image_dir=/Users/19839701/Developer/datasets/images/ppocr_img/imgs_words/en/word_1.png \\\n",
    "--rec_image_shape='3,32,320' \\\n",
    "--rec_char_dict_path=../PaddleOCR/ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f116f-488c-49d5-b215-0b46a604c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../PaddleOCR-2/tools/infer/predict_rec.py --use_gpu=False \\\n",
    "--rec_model_dir=./models/en_number_mobile_v2.0_rec_infer  \\\n",
    "--image_dir=/Users/19839701/Developer/datasets/images/ppocr_img/imgs_words/en/word_1.png \\\n",
    "--rec_image_shape='3,32,320' \\\n",
    "--rec_char_dict_path=../PaddleOCR-2/ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208ff16-5276-498f-ad5a-51461c35a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../PaddleOCR/tools/infer/predict_rec.py --use_gpu=False --use_onnx=True  \\\n",
    "--rec_model_dir=./models/en_number_mobile_v2.0_rec_infer/en_number_mobile_v2.0_rec_infer.onnx  \\\n",
    "--image_dir=/Users/19839701/Developer/datasets/images/ppocr_img/imgs_words/en/word_1.png \\\n",
    "--rec_image_shape='3,32,320' \\\n",
    "--rec_char_dict_path=../PaddleOCR/ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157bc2d-cd7c-4323-9396-2d5015960daf",
   "metadata": {},
   "source": [
    "## en_number_mobile_v2.0_rec_slim_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2696c-0f21-4ab1-a9b3-48edd6cf809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../PaddleOCR/tools/infer/predict_rec.py --use_gpu=False \\\n",
    "--rec_model_dir=./models/en_number_mobile_v2.0_rec_slim_infer  \\\n",
    "--image_dir=/Users/19839701/Developer/datasets/images/ppocr_img/imgs_words/en/word_1.png \\\n",
    "--rec_image_shape='3,32,320' \\\n",
    "--rec_char_dict_path=../PaddleOCR/ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55462bb5-16ad-4849-8a97-98aa3573b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../PaddleOCR/tools/infer/predict_rec.py --use_gpu=False --use_onnx=True \\\n",
    "--rec_model_dir=./models/en_number_mobile_v2.0_rec_slim_infer/en_number_mobile_v2.0_rec_slim_infer.onnx  \\\n",
    "--image_dir=/Users/19839701/Developer/datasets/images/ppocr_img/imgs_words/en/word_1.png \\\n",
    "--rec_image_shape='3,32,320' \\\n",
    "--rec_char_dict_path=../PaddleOCR/ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e177de2-b03f-4524-8f3e-e60220681862",
   "metadata": {},
   "source": [
    "## PaddleOCR-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769cc1ea-345a-4879-b52b-32380d71c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "--rec_char_type=\"en\"\n",
    "--rec_char_dict_path=../PaddleOCR-2/ppocr/utils/en_dict.txt \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad36f7-6799-42e0-b086-da970e6c0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../PaddleOCR-2/tools/infer/predict_rec.py --use_gpu=False \\\n",
    "--rec_model_dir=./models/en_number_mobile_v2.0_rec_slim_infer  \\\n",
    "--image_dir=/Users/19839701/Developer/datasets/images/ppocr_img/imgs_words/en/word_1.png \\\n",
    "--rec_image_shape='3,32,320' \\\n",
    "--rec_char_dict_path=../PaddleOCR-2/ppocr/utils/en_dict.txt \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e25d2-7b60-4c12-8999-a94ec03b0eae",
   "metadata": {},
   "source": [
    "## other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb17713-a5a8-4bc1-85eb-9c46d38342c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run from PaddleOCR root\n",
    "!python ./tools/infer/predict_rec.py --use_gpu=False --use_onnx=True \\\n",
    "--rec_model_dir=../notebooks/models/en_PP-OCRv3_rec_infer/model.onnx  \\\n",
    "--image_dir=/Users/19839701/Developer/datasets/images/ppocr_img/imgs_words/en/word_1.png \\\n",
    "--rec_char_dict_path=./ppocr/utils/en_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5618c61-d292-4160-aef0-a8052846835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at ppocr_keys_v1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430b6e6-8498-4847-b1a2-1b642ac2d973",
   "metadata": {},
   "source": [
    "# onnxruntime det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46408a36-eeb8-4e96-965d-d72f222515fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66619f65-26c2-4234-b0f2-b37b9c1b57ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = './en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.onnx'\n",
    "sess = ort.InferenceSession(model_file_path)\n",
    "sess.get_inputs()\n",
    "input_tensor = sess.get_inputs()[0]\n",
    "input_tensor.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a4841-ff4c-4377-9164-25df3d719c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d3c07e-9e92-4289-b357-bc3072902e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDLE_PATH = \"/Users/user006/Developer/projects/PaddleOCR/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4f43c-725b-494e-91aa-db7c53a21b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(PADDLE_PATH, 'doc/imgs_en/img_12.jpg')\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7fe3e1-3341-4f79-b3b6-92f5fec295b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "\n",
    "print(image.shape)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f4f79-265c-48a1-9e0a-bb5acad755fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_h, src_w, _ = image.shape\n",
    "src_h, src_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31bb6a-6b31-47b6-b034-29a8ae977461",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ae555-c1e5-47e1-8212-d52b88f28ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_type0(img, limit_side_len=960, limit_type='max'):\n",
    "    \"\"\"\n",
    "    resize image to a size multiple of 32 which is required by the network\n",
    "    args:\n",
    "        img(array): array with shape [h, w, c]\n",
    "    return(tuple):\n",
    "        img, (ratio_h, ratio_w)\n",
    "    \"\"\"\n",
    "\n",
    "    h, w, c = img.shape\n",
    "\n",
    "    # limit the max side\n",
    "    if limit_type == 'max':\n",
    "        if max(h, w) > limit_side_len:\n",
    "            if h > w:\n",
    "                ratio = float(limit_side_len) / h\n",
    "            else:\n",
    "                ratio = float(limit_side_len) / w\n",
    "        else:\n",
    "            ratio = 1.\n",
    "    elif limit_type == 'min':\n",
    "        if min(h, w) < limit_side_len:\n",
    "            if h < w:\n",
    "                ratio = float(limit_side_len) / h\n",
    "            else:\n",
    "                ratio = float(limit_side_len) / w\n",
    "        else:\n",
    "            ratio = 1.\n",
    "    elif limit_type == 'resize_long':\n",
    "        ratio = float(limit_side_len) / max(h, w)\n",
    "    else:\n",
    "        raise Exception('not support limit type, image ')\n",
    "    resize_h = int(h * ratio)\n",
    "    resize_w = int(w * ratio)\n",
    "\n",
    "    resize_h = max(int(round(resize_h / 32) * 32), 32)\n",
    "    resize_w = max(int(round(resize_w / 32) * 32), 32)\n",
    "\n",
    "    try:\n",
    "        if int(resize_w) <= 0 or int(resize_h) <= 0:\n",
    "            return None, (None, None)\n",
    "        img = cv2.resize(img, (int(resize_w), int(resize_h)))\n",
    "    except:\n",
    "        print(img.shape, resize_w, resize_h)\n",
    "        sys.exit(0)\n",
    "    ratio_h = resize_h / float(h)\n",
    "    ratio_w = resize_w / float(w)\n",
    "    return img, [ratio_h, ratio_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d81d20-2b95-4c74-a5e8-9cbe489f00cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, [ratio_h, ratio_w] = resize_image_type0(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80cdeb-5bfa-4ce1-83be-2c0049946246",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b04c85b-602a-43e7-867e-d1552170b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_img(img):\n",
    "\n",
    "    scale = np.float32(1.0 / 255.0)\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    shape = (1, 1, 3)\n",
    "    mean = np.array(mean).reshape(shape).astype('float32')\n",
    "    std = np.array(std).reshape(shape).astype('float32')\n",
    "\n",
    "    image = (img.astype('float32') * scale - mean) / std\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fafee6-106c-4479-9114-36f09700e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = norm_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73af91-b62f-4acf-b6ea-30fa69aa7be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.transpose((2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3092c3d-8bab-41a6-9690-1dcb3811e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bba802-3278-4793-a7f0-47db379c5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = img[np.newaxis, :]\n",
    "img_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625430d4-03d7-4348-9549-62f9d82399c1",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbded1eb-d2d9-4a46-b0a2-4387f23c803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensors = None\n",
    "\n",
    "input_dict = {}\n",
    "input_dict[input_tensor.name] = img_batch\n",
    "\n",
    "outputs = sess.run(output_tensors, input_dict)\n",
    "preds = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0a8c8-f58c-4f8b-a357-d05967b1794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(preds[0].transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9cb30-3f57-47a0-835f-690178b05bfc",
   "metadata": {},
   "source": [
    "## Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6679e-0ac9-46e5-bb1e-87368c68451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import pyclipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b147014-ad59-42a3-862b-cb8f388b8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae64ff-cd01-46f0-be18-1303911644f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0, 0, 0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd3458-a4fd-408c-bdc3-0f37955ba1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_h, src_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b85871-08fb-4507-b617-992d53ffa700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unclip(box, unclip_ratio):\n",
    "    poly = Polygon(box)\n",
    "    distance = poly.area * unclip_ratio / poly.length\n",
    "    offset = pyclipper.PyclipperOffset()\n",
    "    offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n",
    "    expanded = np.array(offset.Execute(distance))\n",
    "    return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de5a68-7693-4688-95b0-f3363a8358f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_score_fast(bitmap, _box):\n",
    "    '''\n",
    "    box_score_fast: use bbox mean score as the mean score\n",
    "    '''\n",
    "    h, w = bitmap.shape[:2]\n",
    "    box = _box.copy()\n",
    "    xmin = np.clip(np.floor(box[:, 0].min()).astype(\"int32\"), 0, w - 1)\n",
    "    xmax = np.clip(np.ceil(box[:, 0].max()).astype(\"int32\"), 0, w - 1)\n",
    "    ymin = np.clip(np.floor(box[:, 1].min()).astype(\"int32\"), 0, h - 1)\n",
    "    ymax = np.clip(np.ceil(box[:, 1].max()).astype(\"int32\"), 0, h - 1)\n",
    "\n",
    "    mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)\n",
    "    box[:, 0] = box[:, 0] - xmin\n",
    "    box[:, 1] = box[:, 1] - ymin\n",
    "    cv2.fillPoly(mask, box.reshape(1, -1, 2).astype(\"int32\"), 1)\n",
    "    return cv2.mean(bitmap[ymin:ymax + 1, xmin:xmax + 1], mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b212d-52a7-4662-a156-b0d48f78b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_boxes(contour):\n",
    "    bounding_box = cv2.minAreaRect(contour)\n",
    "    print(bounding_box)\n",
    "    points = sorted(list(cv2.boxPoints(bounding_box)), key=lambda x: x[0])\n",
    "\n",
    "    index_1, index_2, index_3, index_4 = 0, 1, 2, 3\n",
    "    if points[1][1] > points[0][1]:\n",
    "        index_1 = 0\n",
    "        index_4 = 1\n",
    "    else:\n",
    "        index_1 = 1\n",
    "        index_4 = 0\n",
    "    if points[3][1] > points[2][1]:\n",
    "        index_2 = 2\n",
    "        index_3 = 3\n",
    "    else:\n",
    "        index_2 = 3\n",
    "        index_3 = 2\n",
    "\n",
    "    box = [\n",
    "        points[index_1], points[index_2], points[index_3], points[index_4]\n",
    "    ]\n",
    "    return box, min(bounding_box[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57170e-db7b-420c-bf8d-dd082e6dd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes_from_bitmap(pred, _bitmap, dest_width, dest_height):\n",
    "    '''\n",
    "    _bitmap: single map with shape (1, H, W),\n",
    "            whose values are binarized as {0, 1}\n",
    "    '''\n",
    "\n",
    "    max_candidates = 1000\n",
    "    min_size = 3\n",
    "    score_mode = 'fast'\n",
    "    box_thresh = 0.6\n",
    "    unclip_ratio = 1.5\n",
    "\n",
    "    bitmap = _bitmap\n",
    "    height, width = bitmap.shape\n",
    "\n",
    "    outs = cv2.findContours((bitmap * 255).astype(np.uint8), cv2.RETR_LIST,\n",
    "                            cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(outs) == 3:\n",
    "        img, contours, _ = outs[0], outs[1], outs[2]\n",
    "    elif len(outs) == 2:\n",
    "        contours, _ = outs[0], outs[1]\n",
    "\n",
    "    num_contours = min(len(contours), max_candidates)\n",
    "\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    for index in range(num_contours):\n",
    "        contour = contours[index]\n",
    "        points, sside = get_mini_boxes(contour)\n",
    "        if sside < min_size:\n",
    "            continue\n",
    "        points = np.array(points)\n",
    "        if score_mode == \"fast\":\n",
    "            score = box_score_fast(pred, points.reshape(-1, 2))\n",
    "        else:\n",
    "            score = box_score_slow(pred, contour)\n",
    "        if box_thresh > score:\n",
    "            continue\n",
    "\n",
    "        box = unclip(points, unclip_ratio).reshape(-1, 1, 2)\n",
    "        box, sside = get_mini_boxes(box)\n",
    "        if sside < min_size + 2:\n",
    "            continue\n",
    "        box = np.array(box)\n",
    "\n",
    "        box[:, 0] = np.clip(\n",
    "            np.round(box[:, 0] / width * dest_width), 0, dest_width)\n",
    "        box[:, 1] = np.clip(\n",
    "            np.round(box[:, 1] / height * dest_height), 0, dest_height)\n",
    "        boxes.append(box.astype(\"int32\"))\n",
    "        scores.append(score)\n",
    "    return np.array(boxes, dtype=\"int32\"), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb5de8-484d-48a8-b4af-3470c07f7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tag_det_res(self, dt_boxes, image_shape):\n",
    "    img_height, img_width = image_shape[0:2]\n",
    "    dt_boxes_new = []\n",
    "    for box in dt_boxes:\n",
    "        if type(box) is list:\n",
    "            box = np.array(box)\n",
    "        box = order_points_clockwise(box)\n",
    "        box = clip_det_res(box, img_height, img_width)\n",
    "        rect_width = int(np.linalg.norm(box[0] - box[1]))\n",
    "        rect_height = int(np.linalg.norm(box[0] - box[3]))\n",
    "        if rect_width <= 3 or rect_height <= 3:\n",
    "            continue\n",
    "        dt_boxes_new.append(box)\n",
    "    dt_boxes = np.array(dt_boxes_new)\n",
    "    return dt_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fd86a-3409-4f9d-be76-b39af13309dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_proc(preds, thresh=0.3):\n",
    "    pred = preds[:, 0, :, :]\n",
    "    segmentation = pred > thresh\n",
    "\n",
    "    batch_index = 0\n",
    "    mask = segmentation[batch_index]\n",
    "\n",
    "    boxes, scores = boxes_from_bitmap(pred[batch_index], mask, src_w, src_h)\n",
    "\n",
    "    # boxes = filter_tag_det_res(boxes, [src_h, src_w])\n",
    "\n",
    "    return boxes, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdaa99-3f21-46e1-98cc-9eba827dd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes, scores = post_proc(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c1d4dc-82a9-4c69-ba81-7791b2328daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075807d-54e8-499b-b0fd-0f73b7cd61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_draw = image.copy()\n",
    "for i, quad in enumerate(boxes):\n",
    "    cv2.polylines(image_to_draw, [np.int0(quad)], True, (255,0,0), thickness=2)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(image_to_draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8472a741-2083-4e44-a6a0-271e6153e2cb",
   "metadata": {},
   "source": [
    "# onnxruntime rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34672451-f0e2-4e8b-85d4-7126b8df26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b89c9-4fa7-4b88-af38-555aea8ad95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = './models/en_PP-OCRv3_rec_infer/model.onnx'\n",
    "sess = ort.InferenceSession(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1be3d-dd94-4c1a-ab30-1e96a0c33766",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.get_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35adf3a-379b-4b3b-9512-0f5dc07a63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = sess.get_inputs()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66745b-1fdc-4a12-9c20-a43768a875ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd9d73a-870c-47f1-a128-a889d3038aeb",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c4d604-4a2f-439b-b4b3-00f4ce6f3ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_img_batch = np.float32(np.random.rand(1,3,48,320))\n",
    "norm_img_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77d5bd-8446-4555-be83-75538c5c2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = os.path.join(images_path, 'ppocr_img/imgs_words/en/word_1.png')\n",
    "image_path = os.path.join(MRZ_PATH, 'mrz_line_0.png')\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a57f9-3918-47ee-990a-208b87cea841",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "\n",
    "print(image.shape)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddeca33-2f83-43f0-bb7a-87c31a5aff15",
   "metadata": {},
   "source": [
    "<code>\n",
    "ppocr INFO: In PP-OCRv3, rec_image_shape parameter defaults to '3, 48, 320' \n",
    "if you are using recognition model with PP-OCRv2 or an older version, please set --rec_image_shape='3,32,320'\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e75276-f52c-492f-86cf-c55807259406",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.float32(np.random.rand(28,620,3))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f112e1a-d427-4316-929e-eac67a87d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_image_shape = [3, 48, 320]\n",
    "imgC, imgH, imgW = rec_image_shape\n",
    "max_wh_ratio = imgW / imgH\n",
    "max_wh_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199da811-9ef9-439c-a4b4-96cc52177c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = image.shape[:2]\n",
    "h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3e06f-fffe-41f4-bb83-8b6ea0a2ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_ratio = w * 1.0 / h\n",
    "max_wh_ratio = max(max_wh_ratio, wh_ratio)\n",
    "max_wh_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2240a68-bc5c-40ad-901f-39a16a4e2485",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498147f-75ca-468d-98ed-aae92fa5c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_norm_img(img, max_wh_ratio):\n",
    "    imgC, imgH, imgW = rec_image_shape\n",
    "    assert imgC == img.shape[2]\n",
    "    \n",
    "    imgW = int((imgH * max_wh_ratio))\n",
    "    \n",
    "    w = input_tensor.shape[3:][0]\n",
    "    if w is not None and w > 0:\n",
    "        imgW = w\n",
    "        \n",
    "    print(imgW)\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    ratio = w / float(h)\n",
    "    if math.ceil(imgH * ratio) > imgW:\n",
    "        resized_w = imgW\n",
    "    else:\n",
    "        resized_w = int(math.ceil(imgH * ratio))\n",
    "        \n",
    "    print(resized_w)\n",
    "\n",
    "    resized_image = cv2.resize(img, (resized_w, imgH))\n",
    "    resized_image = resized_image.astype('float32')\n",
    "    resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
    "    resized_image -= 0.5\n",
    "    resized_image /= 0.5\n",
    "    padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n",
    "    padding_im[:, :, 0:resized_w] = resized_image\n",
    "    return padding_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0070c9-5cbb-44a5-9547-3f2afbcc8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_image = resize_norm_img(image, max_wh_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762905da-bcc9-41f4-8d20-4ad84a57e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resized_image.shape)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(resized_image.transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a2bb3-51c3-4950-a818-11d30863d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_img_batch = resized_image[np.newaxis, :]\n",
    "norm_img_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62493736-2209-41a5-9cbe-4020414e79fa",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877854ea-f245-4dd9-b858-eaff685fa3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensors = None\n",
    "\n",
    "input_dict = {}\n",
    "input_dict[input_tensor.name] = norm_img_batch\n",
    "\n",
    "outputs = sess.run(output_tensors, input_dict)\n",
    "preds = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f319926-297c-4371-b22a-c2adbcb57d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc0603c-e9da-4c09-b2bb-06f12e3303bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape\n",
    "# batch, num_rec_char, num_voc_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59da18-13df-411c-b49d-bc1811a08c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_idx = preds.argmax(axis=2)\n",
    "preds_prob = preds.max(axis=2)\n",
    "\n",
    "preds_idx.shape, preds_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881637b-70fe-45b6-821c-fdfdf23c7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_idx[0][3] = 6\n",
    "# preds_idx[0][7] = 24\n",
    "preds_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6567c026-1303-4e6b-b792-48a6a40946b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_space_char = False\n",
    "character_dict_path = '../PaddleOCR/ppocr/utils/en_dict.txt'\n",
    "character_str = []\n",
    "\n",
    "with open(character_dict_path, \"rb\") as fin:\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip(\"\\n\").strip(\"\\r\\n\")\n",
    "        character_str.append(line)\n",
    "\n",
    "if use_space_char:\n",
    "    character_str.append(\" \")\n",
    "dict_character = list(character_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85c568-c809-496e-aac8-442458fee8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(character_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91e50a-4250-4eff-8fab-f42ee7acd70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_character = ['blank'] + dict_character\n",
    "dict = {}\n",
    "for i, char in enumerate(dict_character):\n",
    "    dict[char] = i\n",
    "character = dict_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7e5ba-3cc2-4b24-8cb7-2ffebec42c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4a20e-b2b4-400b-9dd6-7b94b9404f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b215fd-29cc-4dce-a486-323c5853b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(text_index, text_prob=None, is_remove_duplicate=False):\n",
    "    \"\"\" convert text-index into text-label. \"\"\"\n",
    "    result_list = []\n",
    "    ignored_tokens = [0]\n",
    "    batch_size = len(text_index)\n",
    "    \n",
    "    for batch_idx in range(batch_size):\n",
    "        selection = np.ones(len(text_index[batch_idx]), dtype=bool)\n",
    "        \n",
    "        if is_remove_duplicate:\n",
    "            selection[1:] = text_index[batch_idx][1:] != text_index[batch_idx][:-1]\n",
    "            \n",
    "        for ignored_token in ignored_tokens:\n",
    "            selection &= text_index[batch_idx] != ignored_token\n",
    "            \n",
    "            \n",
    "        print(text_index[batch_idx][selection])\n",
    "\n",
    "        char_list = [\n",
    "            character[text_id]\n",
    "            for text_id in text_index[batch_idx][selection]\n",
    "        ]\n",
    "        \n",
    "        if text_prob is not None:\n",
    "            conf_list = text_prob[batch_idx][selection]\n",
    "        else:\n",
    "            conf_list = [1] * len(selection)\n",
    "        if len(conf_list) == 0:\n",
    "            conf_list = [0]\n",
    "\n",
    "        text = ''.join(char_list)\n",
    "        result_list.append((text, np.mean(conf_list).tolist()))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de03a05-339b-42bb-b13c-77c5d9727a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = decode(preds_idx, preds_prob, is_remove_duplicate=True)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73824a17-c515-4e00-bff7-06c437402d1b",
   "metadata": {},
   "source": [
    "## Decode details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5d2d6-287d-4f8e-8f39-987b3ec216b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_index = preds_idx\n",
    "batch_idx = 0\n",
    "ignored_token = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb92bd90-6cc0-4a13-9473-d47339785348",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60baffff-df37-4d7a-a8d5-572632b2591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = np.ones(len(text_index[batch_idx]), dtype=bool)\n",
    "selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9445f1e-06ee-4fb4-8545-1560866567f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection[1:] = text_index[batch_idx][1:] != text_index[batch_idx][:-1]\n",
    "selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a044f152-948d-40cc-aa2f-cf651bc598e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection &= text_index[batch_idx] != ignored_token\n",
    "selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975aef18-e6a4-44d4-960a-d052aac29cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_index[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7749ef1-58c5-4c7f-9ec0-1b912fe337a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_index[batch_idx][selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c90ed3-9473-40ef-8a77-48a37ec7b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90513e35-1e06-4f79-ae22-313ce754b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_list = [character[text_id] for text_id in text_index[batch_idx][selection]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee61e23-a8e0-4dbf-bf34-ffafda928df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985a36a-b7ba-4c7f-8703-02c7744aa0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_common",
   "language": "python",
   "name": "dev_common"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
